# AI ëª¨ë“ˆ í†µí•© êµ¬í˜„ ê³„íš

> Phase 3.2: ì´ë¯¸ì§€ â†’ ìë™ ìƒì§• ì¶”ì¶œ íŒŒì´í”„ë¼ì¸

---

## ğŸ“‹ **ì „ì²´ ì•„í‚¤í…ì²˜**

```
ì´ë¯¸ì§€ ì…ë ¥
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ë©€í‹°ëª¨ë‹¬ ë¶„ì„ (ë³‘ë ¬ ì²˜ë¦¬)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â‘  YOLO/COCO (ê°ì²´ ì¸ì‹)            â”‚
â”‚  â‘¡ CLIP (ì¥ë©´/ê°ì • ë¶„ì„)            â”‚
â”‚  â‘¢ EasyOCR (í…ìŠ¤íŠ¸ ì¶”ì¶œ)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
ì‹ í˜¸ í†µí•© & ê°€ì¤‘ì¹˜ ì ìš©
    â†“
ìƒì§• ë¦¬ìŠ¤íŠ¸ (Top 5)
    â†“
P2 (ì£¼ì œ ë§¤í•‘) â†’ P3 (ì»¨í…ìŠ¤íŠ¸) â†’ Scoring
```

---

## ğŸ”§ **ëª¨ë“ˆ 1: YOLO/COCO ê°ì²´ ì¸ì‹**

### **ëª©ì **
- ì´ë¯¸ì§€ ë‚´ ê°ì²´ ê°ì§€ (ì‚¬ëŒ, ì¹¨ëŒ€, ì‹­ìê°€, ì±… ë“±)
- detection_labels_map.csvì™€ ë§¤í•‘

### **ê¸°ìˆ  ìŠ¤íƒ**
```python
# requirements.txt ì¶”ê°€
ultralytics>=8.0.0  # YOLOv8
opencv-python>=4.8.0
torch>=2.0.0
```

### **êµ¬í˜„ ì½”ë“œ (ì˜ˆìƒ)**
```python
# modules/object_detection.py
from ultralytics import YOLO
import cv2
import pandas as pd

class ObjectDetector:
    def __init__(self, model_path='yolov8n.pt', confidence_threshold=0.5):
        """
        YOLOv8 ê°ì²´ ì¸ì‹ ëª¨ë“ˆ
        
        Args:
            model_path: YOLO ëª¨ë¸ ê²½ë¡œ (yolov8n.pt = ê²½ëŸ‰ ëª¨ë¸)
            confidence_threshold: ì‹ ë¢°ë„ ì„ê³„ê°’ (0.5 = 50%)
        """
        self.model = YOLO(model_path)
        self.confidence_threshold = confidence_threshold
        
        # detection_labels_map.csv ë¡œë“œ
        self.label_map = pd.read_csv('data/detection_labels_map.csv')
        
    def detect(self, image_path):
        """
        ì´ë¯¸ì§€ì—ì„œ ê°ì²´ ê°ì§€
        
        Returns:
            List[Dict]: [
                {
                    'coco_class': 'person',
                    'bible_symbol': 'ì‚¬ëŒ',
                    'confidence': 0.92,
                    'bbox': [x1, y1, x2, y2]
                },
                ...
            ]
        """
        # YOLO ì¶”ë¡ 
        results = self.model(image_path)
        
        detected_objects = []
        for result in results:
            for box in result.boxes:
                conf = float(box.conf[0])
                if conf < self.confidence_threshold:
                    continue
                
                # COCO í´ë˜ìŠ¤ â†’ ì„±ê²½ ìƒì§• ë§¤í•‘
                coco_class = result.names[int(box.cls[0])]
                bible_symbol = self._map_to_bible_symbol(coco_class)
                
                if bible_symbol:
                    detected_objects.append({
                        'coco_class': coco_class,
                        'bible_symbol': bible_symbol,
                        'confidence': conf,
                        'bbox': box.xyxy[0].tolist()
                    })
        
        return detected_objects
    
    def _map_to_bible_symbol(self, coco_class):
        """COCO í´ë˜ìŠ¤ â†’ ì„±ê²½ ìƒì§• ë§¤í•‘"""
        mapping = self.label_map[self.label_map['coco_class'] == coco_class]
        if not mapping.empty:
            return mapping.iloc[0]['bible_symbol']
        return None
```

### **ë°ì´í„° ì¤€ë¹„**
```csv
# data/detection_labels_map.csv (ìƒì„± í•„ìš”)
coco_class,bible_symbol,priority
person,ì‚¬ëŒ,1.0
bed,ì¹¨ëŒ€,1.0
book,ì±…,0.9
cross,ì‹­ìê°€,1.0
cup,ì”,0.8
...
```

---

## ğŸ¨ **ëª¨ë“ˆ 2: CLIP ì¥ë©´/ê°ì • ë¶„ì„**

### **ëª©ì **
- ì¥ë©´ ë¶„ë¥˜ (ì‹¤ë‚´/ì‹¤ì™¸, ë°¤/ë‚®)
- ê°ì •/ë¶„ìœ„ê¸° ë¶„ì„ (í‰í™”ë¡œìš´, ìŠ¬í”ˆ, ê¸°ìœ ë“±)
- ìƒì§• í›„ë³´ ìƒì„±

### **ê¸°ìˆ  ìŠ¤íƒ**
```python
# requirements.txt ì¶”ê°€
transformers>=4.30.0
torch>=2.0.0
pillow>=10.0.0
```

### **êµ¬í˜„ ì½”ë“œ (ì˜ˆìƒ)**
```python
# modules/scene_analyzer.py
import torch
from transformers import CLIPProcessor, CLIPModel
from PIL import Image

class SceneAnalyzer:
    def __init__(self, model_name="openai/clip-vit-base-patch32"):
        """
        CLIP ê¸°ë°˜ ì¥ë©´ ë¶„ì„ ëª¨ë“ˆ
        """
        self.model = CLIPModel.from_pretrained(model_name)
        self.processor = CLIPProcessor.from_pretrained(model_name)
        
        # ë¶„ì„ ì¹´í…Œê³ ë¦¬
        self.scene_categories = {
            'location': ['ì‹¤ë‚´', 'ì‹¤ì™¸', 'êµíšŒ', 'ë³‘ì›', 'ìì—°', 'ë„ì‹œ'],
            'time': ['ì•„ì¹¨', 'ë‚®', 'ì €ë…', 'ë°¤', 'ìì •'],
            'mood': ['í‰í™”ë¡œìš´', 'ìŠ¬í”ˆ', 'ê¸°ìœ', 'ê²½ê±´í•œ', 'ê³ ìš”í•œ', 'ì–´ë‘ìš´']
        }
    
    def analyze(self, image_path):
        """
        ì¥ë©´ ë¶„ì„
        
        Returns:
            Dict: {
                'location': ('ë³‘ì›', 0.85),
                'time': ('ë°¤', 0.78),
                'mood': ('ê³ ìš”í•œ', 0.72),
                'suggested_symbols': ['ì¹¨ëŒ€', 'ì–´ë‘ ', 'í‰ê°•']
            }
        """
        image = Image.open(image_path)
        
        results = {}
        for category, labels in self.scene_categories.items():
            # CLIPìœ¼ë¡œ ë¶„ë¥˜
            inputs = self.processor(
                text=labels,
                images=image,
                return_tensors="pt",
                padding=True
            )
            
            outputs = self.model(**inputs)
            logits_per_image = outputs.logits_per_image
            probs = logits_per_image.softmax(dim=1)
            
            # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ë ˆì´ë¸” ì„ íƒ
            max_idx = probs.argmax().item()
            results[category] = (labels[max_idx], float(probs[0][max_idx]))
        
        # ìƒì§• ì œì•ˆ
        results['suggested_symbols'] = self._suggest_symbols(results)
        
        return results
    
    def _suggest_symbols(self, scene_results):
        """ì¥ë©´ ë¶„ì„ ê²°ê³¼ â†’ ìƒì§• ì œì•ˆ"""
        symbols = []
        
        # ê·œì¹™ ê¸°ë°˜ ë§¤í•‘
        location, _ = scene_results['location']
        time, _ = scene_results['time']
        mood, _ = scene_results['mood']
        
        if location == 'ë³‘ì›':
            symbols.extend(['ì¹¨ëŒ€', 'ì¹˜ìœ '])
        if time in ['ë°¤', 'ìì •']:
            symbols.extend(['ì–´ë‘ ', 'ë³„'])
        if mood in ['í‰í™”ë¡œìš´', 'ê³ ìš”í•œ']:
            symbols.extend(['í‰ê°•', 'ì•ˆì‹'])
        
        return list(set(symbols))  # ì¤‘ë³µ ì œê±°
```

---

## ğŸ“ **ëª¨ë“ˆ 3: EasyOCR í…ìŠ¤íŠ¸ ì¶”ì¶œ**

### **ëª©ì **
- ì´ë¯¸ì§€ ë‚´ í•œê¸€/ì˜ì–´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
- ì„±ê²½ êµ¬ì ˆ ê°ì§€ (ì„ íƒì )

### **ê¸°ìˆ  ìŠ¤íƒ**
```python
# requirements.txt ì¶”ê°€
easyocr>=1.7.0
```

### **êµ¬í˜„ ì½”ë“œ (ì˜ˆìƒ)**
```python
# modules/text_extractor.py
import easyocr

class TextExtractor:
    def __init__(self, languages=['ko', 'en']):
        """
        EasyOCR í…ìŠ¤íŠ¸ ì¶”ì¶œ ëª¨ë“ˆ
        """
        self.reader = easyocr.Reader(languages)
    
    def extract(self, image_path):
        """
        í…ìŠ¤íŠ¸ ì¶”ì¶œ
        
        Returns:
            List[Dict]: [
                {
                    'text': 'ìš”í•œë³µìŒ 3:16',
                    'confidence': 0.95,
                    'bbox': [[x1,y1], [x2,y2], [x3,y3], [x4,y4]]
                },
                ...
            ]
        """
        results = self.reader.readtext(image_path)
        
        extracted_texts = []
        for bbox, text, conf in results:
            extracted_texts.append({
                'text': text,
                'confidence': conf,
                'bbox': bbox
            })
        
        return extracted_texts
    
    def detect_bible_verse(self, texts):
        """ì„±ê²½ êµ¬ì ˆ íŒ¨í„´ ê°ì§€"""
        import re
        
        bible_pattern = r'([ê°€-í£]+)\s*(\d+):(\d+)'
        
        for item in texts:
            match = re.search(bible_pattern, item['text'])
            if match:
                return {
                    'book': match.group(1),
                    'chapter': int(match.group(2)),
                    'verse': int(match.group(3)),
                    'confidence': item['confidence']
                }
        
        return None
```

---

## ğŸ”— **ëª¨ë“ˆ 4: ë©€í‹°ëª¨ë‹¬ í†µí•©**

### **ëª©ì **
- 3ê°œ ëª¨ë“ˆì˜ ê²°ê³¼ë¥¼ í†µí•©
- ê°€ì¤‘ì¹˜ ì ìš©í•˜ì—¬ ìµœì¢… ìƒì§• ë¦¬ìŠ¤íŠ¸ ìƒì„±

### **êµ¬í˜„ ì½”ë“œ (ì˜ˆìƒ)**
```python
# modules/multimodal_integrator.py
from typing import List, Dict
from collections import Counter

class MultimodalIntegrator:
    def __init__(self, weights=None):
        """
        ë©€í‹°ëª¨ë‹¬ ì‹ í˜¸ í†µí•©
        
        Args:
            weights: {'object': 0.5, 'scene': 0.3, 'text': 0.2}
        """
        self.weights = weights or {
            'object': 0.5,  # ê°ì²´ ì¸ì‹ ê°€ì¤‘ì¹˜
            'scene': 0.3,   # ì¥ë©´ ë¶„ì„ ê°€ì¤‘ì¹˜
            'text': 0.2     # í…ìŠ¤íŠ¸ ì¶”ì¶œ ê°€ì¤‘ì¹˜
        }
    
    def integrate(self, object_results, scene_results, text_results):
        """
        í†µí•© ë° ìµœì¢… ìƒì§• ë¦¬ìŠ¤íŠ¸ ìƒì„±
        
        Returns:
            List[str]: ['ì¹¨ëŒ€', 'ì–´ë‘ ', 'í‰ê°•', 'ì¹˜ìœ ', 'ê¸°ë„']
        """
        symbol_scores = {}
        
        # 1. ê°ì²´ ì¸ì‹ ê²°ê³¼
        for obj in object_results:
            symbol = obj['bible_symbol']
            score = obj['confidence'] * self.weights['object']
            symbol_scores[symbol] = symbol_scores.get(symbol, 0) + score
        
        # 2. ì¥ë©´ ë¶„ì„ ê²°ê³¼
        for symbol in scene_results.get('suggested_symbols', []):
            score = 0.7 * self.weights['scene']  # ì¥ë©´ ë¶„ì„ì€ ê³ ì • ì‹ ë¢°ë„
            symbol_scores[symbol] = symbol_scores.get(symbol, 0) + score
        
        # 3. í…ìŠ¤íŠ¸ ì¶”ì¶œ ê²°ê³¼ (ì„ íƒì )
        # í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œëœ í‚¤ì›Œë“œë¥¼ ìƒì§•ìœ¼ë¡œ ë³€í™˜
        
        # ì ìˆ˜ìˆœ ì •ë ¬
        sorted_symbols = sorted(
            symbol_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )
        
        # Top 5 ë°˜í™˜
        return [symbol for symbol, score in sorted_symbols[:5]]
```

---

## ğŸš€ **ë©”ì¸ íŒŒì´í”„ë¼ì¸**

```python
# modules/image_pipeline.py
from .object_detection import ObjectDetector
from .scene_analyzer import SceneAnalyzer
from .text_extractor import TextExtractor
from .multimodal_integrator import MultimodalIntegrator

class ImagePipeline:
    def __init__(self):
        self.object_detector = ObjectDetector()
        self.scene_analyzer = SceneAnalyzer()
        self.text_extractor = TextExtractor()
        self.integrator = MultimodalIntegrator()
    
    def process(self, image_path):
        """
        ì´ë¯¸ì§€ â†’ ìƒì§• ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
        
        Returns:
            List[str]: ['ì¹¨ëŒ€', 'ì–´ë‘ ', 'í‰ê°•', 'ì¹˜ìœ ', 'ê¸°ë„']
        """
        # ë³‘ë ¬ ì²˜ë¦¬ (ì„ íƒì )
        object_results = self.object_detector.detect(image_path)
        scene_results = self.scene_analyzer.analyze(image_path)
        text_results = self.text_extractor.extract(image_path)
        
        # í†µí•©
        symbols = self.integrator.integrate(
            object_results,
            scene_results,
            text_results
        )
        
        return symbols
```

---

## ğŸ“¦ **ë””ë ‰í† ë¦¬ êµ¬ì¡°**

```
ANTIGRAVITY/
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ object_detection.py      # YOLO/COCO
â”‚   â”œâ”€â”€ scene_analyzer.py        # CLIP
â”‚   â”œâ”€â”€ text_extractor.py        # EasyOCR
â”‚   â”œâ”€â”€ multimodal_integrator.py # í†µí•©
â”‚   â””â”€â”€ image_pipeline.py        # ë©”ì¸ íŒŒì´í”„ë¼ì¸
â”œâ”€â”€ models/                      # ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ ì €ì¥
â”‚   â”œâ”€â”€ yolov8n.pt
â”‚   â””â”€â”€ clip-vit-base-patch32/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ detection_labels_map.csv # COCO â†’ ì„±ê²½ ìƒì§• ë§¤í•‘
â”‚   â””â”€â”€ ...
â””â”€â”€ bible_gpt_dashboard.py       # ëŒ€ì‹œë³´ë“œ í†µí•©
```

---

## â±ï¸ **êµ¬í˜„ ì¼ì •**

### **Day 1 (4-5ì‹œê°„)**
- [ ] ëª¨ë“ˆ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
- [ ] ObjectDetector êµ¬í˜„ (YOLO)
- [ ] detection_labels_map.csv ìƒì„±
- [ ] í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë¡œ ê²€ì¦

### **Day 2 (3-4ì‹œê°„)**
- [ ] SceneAnalyzer êµ¬í˜„ (CLIP)
- [ ] TextExtractor êµ¬í˜„ (EasyOCR)
- [ ] MultimodalIntegrator êµ¬í˜„
- [ ] ImagePipeline í†µí•©

### **Day 3 (2-3ì‹œê°„)**
- [ ] ëŒ€ì‹œë³´ë“œ í†µí•©
- [ ] ì„±ëŠ¥ ìµœì í™”
- [ ] End-to-End í…ŒìŠ¤íŠ¸

---

## ğŸ¯ **ì„±ëŠ¥ ëª©í‘œ**

- **ì •í™•ë„**: Top-5 ìƒì§• ì¤‘ 1ê°œ ì´ìƒ ê´€ë ¨ì„± ìˆìŒ (80% ì´ìƒ)
- **ì†ë„**: p95 < 800ms (ì •ë³¸ MD ìš”êµ¬ì‚¬í•­)
- **ê²½ëŸ‰í™”**: ëª¨ë°”ì¼ ë””ë°”ì´ìŠ¤ì—ì„œë„ ì‹¤í–‰ ê°€ëŠ¥ (ONNX ë³€í™˜)

---

## ğŸ“ **ë‹¤ìŒ ë‹¨ê³„**

1. **requirements.txt ì—…ë°ì´íŠ¸**
2. **modules/ ë””ë ‰í† ë¦¬ ìƒì„±**
3. **ObjectDetectorë¶€í„° ìˆœì°¨ êµ¬í˜„**
4. **ê° ëª¨ë“ˆë³„ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸**
5. **ëŒ€ì‹œë³´ë“œ í†µí•©**
