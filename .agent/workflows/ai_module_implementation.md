# AI ëª¨ë“ˆ í†µí•© êµ¬í˜„ ê³„íš (v2.0 - ì •ë³¸ 9ì¥ ê¸°ì¤€)

> Phase 3.2: ì´ë¯¸ì§€ â†’ ìë™ ìƒì§• ì¶”ì¶œ íŒŒì´í”„ë¼ì¸  
> **ê¸°ì¤€ ë¬¸ì„œ**: old_AI_setting.md (ì •ë³¸ 9ì¥ AI ë¡œì§)

---

## ğŸ“‹ **ì „ì²´ ì•„í‚¤í…ì²˜ (ì •ë³¸ ê¸°ì¤€)**

```
ì´ë¯¸ì§€ ì…ë ¥ + EXIF + ì»¨í…ìŠ¤íŠ¸
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ì „ì²˜ë¦¬ & ë¯¼ê° ë¼ìš°íŒ…                â”‚
â”‚  - EXIF ì •ê·œí™”                       â”‚
â”‚  - ë¦¬ì‚¬ì´ì¦ˆ + ë ˆí„°ë°•ìŠ¤               â”‚
â”‚  - safety_flags ìƒì„±                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Perception (ì§€ê° ë ˆì´ì–´)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â‘  ê°ì²´ ì¸ì‹ (YOLO)                 â”‚
â”‚  â‘¡ ì¥ë©´ ë¶„ë¥˜ (CLIP)                 â”‚
â”‚  â‘¢ ê°ì • ì¸ì‹ (MobileNetV2) â­ ì¶”ê°€   â”‚
â”‚  â‘£ OCR (EasyOCR)                    â”‚
â”‚  â‘¤ ëœë“œë§ˆí¬ ì¸ì‹ (ì„ íƒ)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  P1: ìƒì§• ë§¤í•‘ (46ê°œ ìƒì§• ì²´ê³„) â­   â”‚
â”‚  - 5ê°œ ì½”ì–´ êµ° êµ¬ì¡°                  â”‚
â”‚  - ê°ì²´/ì¥ë©´/ê°ì • â†’ ìƒì§• ë³€í™˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  P2: ì£¼ì œ ë§¤í•‘ (120Ã—24 ë§¤íŠ¸ë¦­ìŠ¤)    â”‚
â”‚  - ìƒì§• â†’ 24ê°œ ì£¼ì œ                 â”‚
â”‚  - S1-S4 ì ìˆ˜ ê³„ì‚° â­                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  P3: ì»¨í…ìŠ¤íŠ¸ ë³´ì •                   â”‚
â”‚  - ì‹œê°„/ì¥ì†Œ/ì‹œê°íŠ¹ì§• ë³´ì •           â”‚
â”‚  - ì‹ í•™ì  ì•ˆì „ ê·œì¹™ ì ìš©             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
ë²¡í„° ê²€ìƒ‰ â†’ ì¬ë­í‚¹ â†’ Top-1/2 êµ¬ì ˆ
```

---

## ğŸ”§ **ëª¨ë“ˆ 0: ì „ì²˜ë¦¬ & ë¯¼ê° ë¼ìš°íŒ…** â­ ì‹ ê·œ ì¶”ê°€

### **ëª©ì **
- Perception ì‹¤í–‰ ì „ ì…ë ¥ í‘œì¤€í™”
- ë¯¼ê° ìƒí™© ì¡°ê¸° íƒì§€ ë° ì•ˆì „ í”Œë¡œìš° ë¼ìš°íŒ…

### **ê¸°ìˆ  ìŠ¤íƒ**
```python
# requirements.txt ì¶”ê°€
pillow>=10.0.0
exifread>=3.0.0
```

### **êµ¬í˜„ ì½”ë“œ**
```python
# modules/preprocessor.py
from PIL import Image
import exifread
from datetime import datetime

class ImagePreprocessor:
    def __init__(self, target_size=2048):
        """
        ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë° EXIF ì •ê·œí™”
        
        Args:
            target_size: ê¸´ ë³€ ê¸°ì¤€ ë¦¬ì‚¬ì´ì¦ˆ í¬ê¸°
        """
        self.target_size = target_size
    
    def preprocess(self, image_path):
        """
        ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
        
        Returns:
            Dict: {
                'image': PIL.Image,
                'exif_time': datetime,
                'orientation': int,
                'metadata': dict
            }
        """
        # 1. EXIF ì½ê¸°
        with open(image_path, 'rb') as f:
            tags = exifread.process_file(f)
        
        # 2. ì´ë¯¸ì§€ ë¡œë“œ ë° íšŒì „ ë³´ì •
        image = Image.open(image_path)
        orientation = self._get_orientation(tags)
        image = self._apply_orientation(image, orientation)
        
        # 3. ë¦¬ì‚¬ì´ì¦ˆ + ë ˆí„°ë°•ìŠ¤
        image = self._resize_letterbox(image)
        
        # 4. ì´¬ì˜ ì‹œê° ì¶”ì¶œ
        exif_time = self._extract_datetime(tags)
        
        return {
            'image': image,
            'exif_time': exif_time,
            'orientation': orientation,
            'metadata': {
                'exif_present': len(tags) > 0,
                'time_source': 'exif' if exif_time else 'upload'
            }
        }
    
    def _resize_letterbox(self, image):
        """ê¸´ ë³€ ê¸°ì¤€ ë¦¬ì‚¬ì´ì¦ˆ + ë ˆí„°ë°•ìŠ¤ íŒ¨ë”©"""
        w, h = image.size
        scale = self.target_size / max(w, h)
        new_w, new_h = int(w * scale), int(h * scale)
        
        # ë¦¬ì‚¬ì´ì¦ˆ
        image = image.resize((new_w, new_h), Image.LANCZOS)
        
        # ë ˆí„°ë°•ìŠ¤ (ì •ì‚¬ê°í˜•ìœ¼ë¡œ)
        new_image = Image.new('RGB', (self.target_size, self.target_size), (0, 0, 0))
        paste_x = (self.target_size - new_w) // 2
        paste_y = (self.target_size - new_h) // 2
        new_image.paste(image, (paste_x, paste_y))
        
        return new_image
    
    def _get_orientation(self, tags):
        """EXIF Orientation ì¶”ì¶œ"""
        if 'Image Orientation' in tags:
            return int(str(tags['Image Orientation']))
        return 1
    
    def _apply_orientation(self, image, orientation):
        """Orientation ê¸°ë°˜ íšŒì „ ë³´ì •"""
        if orientation == 3:
            return image.rotate(180, expand=True)
        elif orientation == 6:
            return image.rotate(270, expand=True)
        elif orientation == 8:
            return image.rotate(90, expand=True)
        return image
    
    def _extract_datetime(self, tags):
        """ì´¬ì˜ ì‹œê° ì¶”ì¶œ"""
        if 'EXIF DateTimeOriginal' in tags:
            dt_str = str(tags['EXIF DateTimeOriginal'])
            return datetime.strptime(dt_str, '%Y:%m:%d %H:%M:%S')
        return None


# modules/safety_router.py
class SafetyRouter:
    def __init__(self):
        """
        ë¯¼ê° ë¼ìš°íŒ… ëª¨ë“ˆ
        Perception ì´ì „ì— ë¯¼ê° ì‹ í˜¸ íƒì§€
        """
        self.sensitive_keywords = {
            'medical': ['ë³‘ì›', 'ER', 'ì‘ê¸‰ì‹¤', 'ë³‘ì‹¤', 'ICU'],
            'funeral': ['ì¥ë¡€', 'ì¡°ë¬¸', 'ë¬˜ì§€', 'ê´€', 'í—Œí™”'],
            'child': ['ìœ ì•„', 'ì–´ë¦°ì´', 'ìœ ì¹˜ì›', 'ì´ˆë“±í•™êµ'],
            'politics': ['ì‹œìœ„', 'ì§‘íšŒ', 'ì •ë‹¹', 'ì„ ê±°']
        }
    
    def route(self, image, ocr_text=''):
        """
        ë¯¼ê° ë¼ìš°íŒ… ì‹¤í–‰
        
        Returns:
            List[str]: safety_flags
        """
        flags = []
        
        # OCR ê¸°ë°˜ ë¯¼ê° í‚¤ì›Œë“œ íƒì§€
        for category, keywords in self.sensitive_keywords.items():
            if any(kw in ocr_text for kw in keywords):
                flags.append(f'{category}_detected')
        
        # ì´ë¯¸ì§€ ê¸°ë°˜ íƒì§€ëŠ” Perception ì´í›„ ë³´ì™„
        
        return flags
```

### **ë°ì´í„° ì¤€ë¹„**
- ì—†ìŒ (ì „ì²˜ë¦¬ ë¡œì§ë§Œ í•„ìš”)

---

## ğŸ”§ **ëª¨ë“ˆ 1: ê°ì²´ ì¸ì‹ (YOLO/COCO)**

### **ëª©ì **
- ì´ë¯¸ì§€ ë‚´ ê°ì²´ ê°ì§€ (ì‚¬ëŒ, ì¹¨ëŒ€, ì‹­ìê°€, ì±… ë“±)
- detection_labels_map.csvì™€ ë§¤í•‘

### **ê¸°ìˆ  ìŠ¤íƒ**
```python
# requirements.txt ì¶”ê°€
ultralytics>=8.0.0  # YOLOv8
opencv-python>=4.8.0
torch>=2.0.0
```

### **êµ¬í˜„ ì½”ë“œ (ì˜ˆìƒ)**
```python
# modules/object_detection.py
from ultralytics import YOLO
import cv2
import pandas as pd

class ObjectDetector:
    def __init__(self, model_path='yolov8n.pt', confidence_threshold=0.5):
        """
        YOLOv8 ê°ì²´ ì¸ì‹ ëª¨ë“ˆ
        
        Args:
            model_path: YOLO ëª¨ë¸ ê²½ë¡œ (yolov8n.pt = ê²½ëŸ‰ ëª¨ë¸)
            confidence_threshold: ì‹ ë¢°ë„ ì„ê³„ê°’ (0.5 = 50%)
        """
        self.model = YOLO(model_path)
        self.confidence_threshold = confidence_threshold
        
        # detection_labels_map.csv ë¡œë“œ
        self.label_map = pd.read_csv('data/detection_labels_map.csv')
        
    def detect(self, image_path):
        """
        ì´ë¯¸ì§€ì—ì„œ ê°ì²´ ê°ì§€
        
        Returns:
            List[Dict]: [
                {
                    'coco_class': 'person',
                    'bible_symbol': 'ì‚¬ëŒ',
                    'confidence': 0.92,
                    'bbox': [x1, y1, x2, y2]
                },
                ...
            ]
        """
        # YOLO ì¶”ë¡ 
        results = self.model(image_path)
        
        detected_objects = []
        for result in results:
            for box in result.boxes:
                conf = float(box.conf[0])
                if conf < self.confidence_threshold:
                    continue
                
                # COCO í´ë˜ìŠ¤ â†’ ì„±ê²½ ìƒì§• ë§¤í•‘
                coco_class = result.names[int(box.cls[0])]
                bible_symbol = self._map_to_bible_symbol(coco_class)
                
                if bible_symbol:
                    detected_objects.append({
                        'coco_class': coco_class,
                        'bible_symbol': bible_symbol,
                        'confidence': conf,
                        'bbox': box.xyxy[0].tolist()
                    })
        
        return detected_objects
    
    def _map_to_bible_symbol(self, coco_class):
        """COCO í´ë˜ìŠ¤ â†’ ì„±ê²½ ìƒì§• ë§¤í•‘"""
        mapping = self.label_map[self.label_map['coco_class'] == coco_class]
        if not mapping.empty:
            return mapping.iloc[0]['bible_symbol']
        return None
```

### **ë°ì´í„° ì¤€ë¹„**
```csv
# data/detection_labels_map.csv (ìƒì„± í•„ìš”)
coco_class,bible_symbol,priority
person,ì‚¬ëŒ,1.0
bed,ì¹¨ëŒ€,1.0
book,ì±…,0.9
cross,ì‹­ìê°€,1.0
cup,ì”,0.8
...
```

---

## ğŸ¨ **ëª¨ë“ˆ 2: CLIP ì¥ë©´/ê°ì • ë¶„ì„**

### **ëª©ì **
- ì¥ë©´ ë¶„ë¥˜ (ì‹¤ë‚´/ì‹¤ì™¸, ë°¤/ë‚®)
- ê°ì •/ë¶„ìœ„ê¸° ë¶„ì„ (í‰í™”ë¡œìš´, ìŠ¬í”ˆ, ê¸°ìœ ë“±)
- ìƒì§• í›„ë³´ ìƒì„±

### **ê¸°ìˆ  ìŠ¤íƒ**
```python
# requirements.txt ì¶”ê°€
transformers>=4.30.0
torch>=2.0.0
pillow>=10.0.0
```

### **êµ¬í˜„ ì½”ë“œ (ì˜ˆìƒ)**
```python
# modules/scene_analyzer.py
import torch
from transformers import CLIPProcessor, CLIPModel
from PIL import Image

class SceneAnalyzer:
    def __init__(self, model_name="openai/clip-vit-base-patch32"):
        """
        CLIP ê¸°ë°˜ ì¥ë©´ ë¶„ì„ ëª¨ë“ˆ
        """
        self.model = CLIPModel.from_pretrained(model_name)
        self.processor = CLIPProcessor.from_pretrained(model_name)
        
        # ë¶„ì„ ì¹´í…Œê³ ë¦¬
        self.scene_categories = {
            'location': ['ì‹¤ë‚´', 'ì‹¤ì™¸', 'êµíšŒ', 'ë³‘ì›', 'ìì—°', 'ë„ì‹œ'],
            'time': ['ì•„ì¹¨', 'ë‚®', 'ì €ë…', 'ë°¤', 'ìì •'],
            'mood': ['í‰í™”ë¡œìš´', 'ìŠ¬í”ˆ', 'ê¸°ìœ', 'ê²½ê±´í•œ', 'ê³ ìš”í•œ', 'ì–´ë‘ìš´']
        }
    
    def analyze(self, image_path):
        """
        ì¥ë©´ ë¶„ì„
        
        Returns:
            Dict: {
                'location': ('ë³‘ì›', 0.85),
                'time': ('ë°¤', 0.78),
                'mood': ('ê³ ìš”í•œ', 0.72),
                'suggested_symbols': ['ì¹¨ëŒ€', 'ì–´ë‘ ', 'í‰ê°•']
            }
        """
        image = Image.open(image_path)
        
        results = {}
        for category, labels in self.scene_categories.items():
            # CLIPìœ¼ë¡œ ë¶„ë¥˜
            inputs = self.processor(
                text=labels,
                images=image,
                return_tensors="pt",
                padding=True
            )
            
            outputs = self.model(**inputs)
            logits_per_image = outputs.logits_per_image
            probs = logits_per_image.softmax(dim=1)
            
            # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ë ˆì´ë¸” ì„ íƒ
            max_idx = probs.argmax().item()
            results[category] = (labels[max_idx], float(probs[0][max_idx]))
        
        # ìƒì§• ì œì•ˆ
        results['suggested_symbols'] = self._suggest_symbols(results)
        
        return results
    
    def _suggest_symbols(self, scene_results):
        """ì¥ë©´ ë¶„ì„ ê²°ê³¼ â†’ ìƒì§• ì œì•ˆ"""
        symbols = []
        
        # ê·œì¹™ ê¸°ë°˜ ë§¤í•‘
        location, _ = scene_results['location']
        time, _ = scene_results['time']
        mood, _ = scene_results['mood']
        
        if location == 'ë³‘ì›':
            symbols.extend(['ì¹¨ëŒ€', 'ì¹˜ìœ '])
        if time in ['ë°¤', 'ìì •']:
            symbols.extend(['ì–´ë‘ ', 'ë³„'])
        if mood in ['í‰í™”ë¡œìš´', 'ê³ ìš”í•œ']:
            symbols.extend(['í‰ê°•', 'ì•ˆì‹'])
        
        return list(set(symbols))  # ì¤‘ë³µ ì œê±°
```

---

## ğŸ“ **ëª¨ë“ˆ 3: EasyOCR í…ìŠ¤íŠ¸ ì¶”ì¶œ**

### **ëª©ì **
- ì´ë¯¸ì§€ ë‚´ í•œê¸€/ì˜ì–´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
- ì„±ê²½ êµ¬ì ˆ ê°ì§€ (ì„ íƒì )

### **ê¸°ìˆ  ìŠ¤íƒ**
```python
# requirements.txt ì¶”ê°€
easyocr>=1.7.0
```

### **êµ¬í˜„ ì½”ë“œ (ì˜ˆìƒ)**
```python
# modules/text_extractor.py
import easyocr

class TextExtractor:
    def __init__(self, languages=['ko', 'en']):
        """
        EasyOCR í…ìŠ¤íŠ¸ ì¶”ì¶œ ëª¨ë“ˆ
        """
        self.reader = easyocr.Reader(languages)
    
    def extract(self, image_path):
        """
        í…ìŠ¤íŠ¸ ì¶”ì¶œ
        
        Returns:
            List[Dict]: [
                {
                    'text': 'ìš”í•œë³µìŒ 3:16',
                    'confidence': 0.95,
                    'bbox': [[x1,y1], [x2,y2], [x3,y3], [x4,y4]]
                },
                ...
            ]
        """
        results = self.reader.readtext(image_path)
        
        extracted_texts = []
        for bbox, text, conf in results:
            extracted_texts.append({
                'text': text,
                'confidence': conf,
                'bbox': bbox
            })
        
        return extracted_texts
    
    def detect_bible_verse(self, texts):
        """ì„±ê²½ êµ¬ì ˆ íŒ¨í„´ ê°ì§€"""
        import re
        
        bible_pattern = r'([ê°€-í£]+)\s*(\d+):(\d+)'
        
        for item in texts:
            match = re.search(bible_pattern, item['text'])
            if match:
                return {
                    'book': match.group(1),
                    'chapter': int(match.group(2)),
                    'verse': int(match.group(3)),
                    'confidence': item['confidence']
                }
        
        return None
```

---

## ğŸ”— **ëª¨ë“ˆ 4: ë©€í‹°ëª¨ë‹¬ í†µí•©**

### **ëª©ì **
- 3ê°œ ëª¨ë“ˆì˜ ê²°ê³¼ë¥¼ í†µí•©
- ê°€ì¤‘ì¹˜ ì ìš©í•˜ì—¬ ìµœì¢… ìƒì§• ë¦¬ìŠ¤íŠ¸ ìƒì„±

### **êµ¬í˜„ ì½”ë“œ (ì˜ˆìƒ)**
```python
# modules/multimodal_integrator.py
from typing import List, Dict
from collections import Counter

class MultimodalIntegrator:
    def __init__(self, weights=None):
        """
        ë©€í‹°ëª¨ë‹¬ ì‹ í˜¸ í†µí•©
        
        Args:
            weights: {'object': 0.5, 'scene': 0.3, 'text': 0.2}
        """
        self.weights = weights or {
            'object': 0.5,  # ê°ì²´ ì¸ì‹ ê°€ì¤‘ì¹˜
            'scene': 0.3,   # ì¥ë©´ ë¶„ì„ ê°€ì¤‘ì¹˜
            'text': 0.2     # í…ìŠ¤íŠ¸ ì¶”ì¶œ ê°€ì¤‘ì¹˜
        }
    
    def integrate(self, object_results, scene_results, text_results):
        """
        í†µí•© ë° ìµœì¢… ìƒì§• ë¦¬ìŠ¤íŠ¸ ìƒì„±
        
        Returns:
            List[str]: ['ì¹¨ëŒ€', 'ì–´ë‘ ', 'í‰ê°•', 'ì¹˜ìœ ', 'ê¸°ë„']
        """
        symbol_scores = {}
        
        # 1. ê°ì²´ ì¸ì‹ ê²°ê³¼
        for obj in object_results:
            symbol = obj['bible_symbol']
            score = obj['confidence'] * self.weights['object']
            symbol_scores[symbol] = symbol_scores.get(symbol, 0) + score
        
        # 2. ì¥ë©´ ë¶„ì„ ê²°ê³¼
        for symbol in scene_results.get('suggested_symbols', []):
            score = 0.7 * self.weights['scene']  # ì¥ë©´ ë¶„ì„ì€ ê³ ì • ì‹ ë¢°ë„
            symbol_scores[symbol] = symbol_scores.get(symbol, 0) + score
        
        # 3. í…ìŠ¤íŠ¸ ì¶”ì¶œ ê²°ê³¼ (ì„ íƒì )
        # í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œëœ í‚¤ì›Œë“œë¥¼ ìƒì§•ìœ¼ë¡œ ë³€í™˜
        
        # ì ìˆ˜ìˆœ ì •ë ¬
        sorted_symbols = sorted(
            symbol_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )
        
        # Top 5 ë°˜í™˜
        return [symbol for symbol, score in sorted_symbols[:5]]
```

---

## ğŸš€ **ë©”ì¸ íŒŒì´í”„ë¼ì¸**

```python
# modules/image_pipeline.py
from .object_detection import ObjectDetector
from .scene_analyzer import SceneAnalyzer
from .text_extractor import TextExtractor
from .multimodal_integrator import MultimodalIntegrator

class ImagePipeline:
    def __init__(self):
        self.object_detector = ObjectDetector()
        self.scene_analyzer = SceneAnalyzer()
        self.text_extractor = TextExtractor()
        self.integrator = MultimodalIntegrator()
    
    def process(self, image_path):
        """
        ì´ë¯¸ì§€ â†’ ìƒì§• ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
        
        Returns:
            List[str]: ['ì¹¨ëŒ€', 'ì–´ë‘ ', 'í‰ê°•', 'ì¹˜ìœ ', 'ê¸°ë„']
        """
        # ë³‘ë ¬ ì²˜ë¦¬ (ì„ íƒì )
        object_results = self.object_detector.detect(image_path)
        scene_results = self.scene_analyzer.analyze(image_path)
        text_results = self.text_extractor.extract(image_path)
        
        # í†µí•©
        symbols = self.integrator.integrate(
            object_results,
            scene_results,
            text_results
        )
        
        return symbols
```

---

## ğŸ“¦ **ë””ë ‰í† ë¦¬ êµ¬ì¡°**

```
ANTIGRAVITY/
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ object_detection.py      # YOLO/COCO
â”‚   â”œâ”€â”€ scene_analyzer.py        # CLIP
â”‚   â”œâ”€â”€ text_extractor.py        # EasyOCR
â”‚   â”œâ”€â”€ multimodal_integrator.py # í†µí•©
â”‚   â””â”€â”€ image_pipeline.py        # ë©”ì¸ íŒŒì´í”„ë¼ì¸
â”œâ”€â”€ models/                      # ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ ì €ì¥
â”‚   â”œâ”€â”€ yolov8n.pt
â”‚   â””â”€â”€ clip-vit-base-patch32/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ detection_labels_map.csv # COCO â†’ ì„±ê²½ ìƒì§• ë§¤í•‘
â”‚   â””â”€â”€ ...
â””â”€â”€ bible_gpt_dashboard.py       # ëŒ€ì‹œë³´ë“œ í†µí•©
```

---

## â±ï¸ **êµ¬í˜„ ì¼ì •**

### **Day 1 (4-5ì‹œê°„)**
- [ ] ëª¨ë“ˆ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
- [ ] ObjectDetector êµ¬í˜„ (YOLO)
- [ ] detection_labels_map.csv ìƒì„±
- [ ] í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë¡œ ê²€ì¦

### **Day 2 (3-4ì‹œê°„)**
- [ ] SceneAnalyzer êµ¬í˜„ (CLIP)
- [ ] TextExtractor êµ¬í˜„ (EasyOCR)
- [ ] MultimodalIntegrator êµ¬í˜„
- [ ] ImagePipeline í†µí•©

### **Day 3 (2-3ì‹œê°„)**
- [ ] ëŒ€ì‹œë³´ë“œ í†µí•©
- [ ] ì„±ëŠ¥ ìµœì í™”
- [ ] End-to-End í…ŒìŠ¤íŠ¸

---

## ğŸ¯ **ì„±ëŠ¥ ëª©í‘œ**

- **ì •í™•ë„**: Top-5 ìƒì§• ì¤‘ 1ê°œ ì´ìƒ ê´€ë ¨ì„± ìˆìŒ (80% ì´ìƒ)
- **ì†ë„**: p95 < 800ms (ì •ë³¸ MD ìš”êµ¬ì‚¬í•­)
- **ê²½ëŸ‰í™”**: ëª¨ë°”ì¼ ë””ë°”ì´ìŠ¤ì—ì„œë„ ì‹¤í–‰ ê°€ëŠ¥ (ONNX ë³€í™˜)

---

## ğŸ“ **ë‹¤ìŒ ë‹¨ê³„**

1. **requirements.txt ì—…ë°ì´íŠ¸**
2. **modules/ ë””ë ‰í† ë¦¬ ìƒì„±**
3. **ObjectDetectorë¶€í„° ìˆœì°¨ êµ¬í˜„**
4. **ê° ëª¨ë“ˆë³„ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸**
5. **ëŒ€ì‹œë³´ë“œ í†µí•©**
